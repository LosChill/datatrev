[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datatrev",
    "section": "",
    "text": "A creative data studio.\n\n\nData Visualizations, Explorations, Reporting, Dashboards, Scripts, you name it we do it!\nNo questions asked, no judgement passed. No job too small! Se habla español."
  },
  {
    "objectID": "index.html#affordable-data-solutions",
    "href": "index.html#affordable-data-solutions",
    "title": "datatrev",
    "section": "",
    "text": "Data Visualizations, Explorations, Reporting, Dashboards, Scripts, you name it we do it!\nNo questions asked, no judgement passed. No job too small! Se habla español."
  },
  {
    "objectID": "datatrev.html",
    "href": "datatrev.html",
    "title": "datatrev",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "datatrev.html#quarto",
    "href": "datatrev.html#quarto",
    "title": "datatrev",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "tidytuesday2024/tidy20.html",
    "href": "tidytuesday2024/tidy20.html",
    "title": "20",
    "section": "",
    "text": "In my first ever #tidytuesday we are taking a look at a coffee survey. I’ll make a note up front that I am prioritizing self education over analytical integrity here. Curiosity is in the drivers seat, so we are going to see where this takes us. Anything I present here is not intended to draw any conclusions or correlations (I tried, and any statistically sound correlation just wasn’t there with this data).",
    "crumbs": [
      "Tidy Tuesday 2024",
      "20"
    ]
  },
  {
    "objectID": "tidytuesday2024/tidy20.html#overview",
    "href": "tidytuesday2024/tidy20.html#overview",
    "title": "20",
    "section": "",
    "text": "In my first ever #tidytuesday we are taking a look at a coffee survey. I’ll make a note up front that I am prioritizing self education over analytical integrity here. Curiosity is in the drivers seat, so we are going to see where this takes us. Anything I present here is not intended to draw any conclusions or correlations (I tried, and any statistically sound correlation just wasn’t there with this data).",
    "crumbs": [
      "Tidy Tuesday 2024",
      "20"
    ]
  },
  {
    "objectID": "tidytuesday2024/tidy20.html#etl-let",
    "href": "tidytuesday2024/tidy20.html#etl-let",
    "title": "20",
    "section": "ETL (LET)",
    "text": "ETL (LET)\n\nLoad Up\nNothing crazy.\n\nlibrary(tidytuesdayR)\nsuppressMessages(library(tidyverse))\nlibrary(ggforce)\nlibrary(knitr)\n\n\n\nExtract\nI tried the tidytuesdayR::tt_load method and kept getting an error, so we url it.\n\ncoffee_survey &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-05-14/coffee_survey.csv', show_col_types = FALSE)\n\n\n\nTransform\nNice initial cleaning. I decided straight away that my analysis would not be considering open-response answers. I organized the rest of the columns into two categories: multiple response, and factored. Here I factor the latter group. Later I’ll decide what to do with the former.\nFinally I have the pleasure of correcting the “spend” column typo. People need to stop using “spend” as a noun. It is a verb. The noun is “expenditure”.\n\n# Trim out free-text data, fix \"spend\" typo \n# \"spend\" is not a noun, stop trying to make it one\nclean_tib &lt;- coffee_survey %&gt;%\n  select(-matches(\"other|specify|coffee_a|coffee_b|coffee_c|coffee_d|prefer\")) %&gt;%\n  rename(monthly_expenditure = total_spend) %&gt;%\n  mutate(across(everything(), ~ if(is.character(.)) tolower(.) else .)) %&gt;%\n  mutate(across(c(age, cups, favorite, style, strength, roast_level, caffeine,\n                  expertise, wfh, monthly_expenditure, taste, know_source,\n                  most_paid, most_willing, value_cafe, spent_equipment,\n                  value_equipment, gender, education_level, ethnicity_race,\n                  employment_status, number_children, political_affiliation\n                  ), as.factor\n                )\n         )",
    "crumbs": [
      "Tidy Tuesday 2024",
      "20"
    ]
  },
  {
    "objectID": "tidytuesday2024/tidy20.html#eda",
    "href": "tidytuesday2024/tidy20.html#eda",
    "title": "20",
    "section": "EDA",
    "text": "EDA\nI saw the multiple-response columns and decided to try to learn more about the best ways to handle these. I could make each cell a vector, turning the column into a list of character vectors, or I could try one-hot encoding. I brought in the caret library and played around with dummy variables enough to get a basic transformation of all the multiple-response columns into multiple single boolean columns for each response.\nThis was messy and I didn’t love my function but it was enough to try a Cramers V analysis comparing every column pair for possible correlation. I would love to have found ANY surprise coffee trend that correlated to personal data (like maybe all the cinnamon takers are all unemployed), but the highest chi square was .5 and nothing compelling. So I gave up before I started showing more bias than I had already in wanting to find something.\nI may clean that code up and include it later.\n\nStrip Columns\nBut for now I stripped out all of those multiple-response columns. This allowed me to drop all NA cells. These columns were the questions that many people seem to have just skipped, so stripping them saved more observations.\n\n# List of columns with multi-value cells\nhot_columns &lt;- c(\"where_drink\",\n                 \"brew\",\n                 \"purchase\",\n                 \"additions\",\n                 \"dairy\",\n                 \"sweetener\",\n                 \"why_drink\"\n                 )\n\n# Drop multi-value columns before dropping NA cells\nstrip_tib &lt;- clean_tib %&gt;%\n  select(-all_of(hot_columns)) %&gt;%\n  drop_na()\n\n\n\nFinding Our Target\nWhat interested me is the self-reported coffee expertise. These questions are such a labyrinth of psychology I don’t know what analysts really use them for other than profiling over confidence maybe? I thought Dunning-Kruger did away with the presumed value of self-assessed expertise.\nWhat other question might be fun to compare to this self-expertise?\n“Do you know the source of your coffee”\nLets take a look. A jitter-box plot will show this nicely.\n\nexpertiseVknowsource_tib &lt;- strip_tib %&gt;%\n  select(expertise, know_source)\n\nset.seed(357)\njitter &lt;- position_jitter(width = 0.25, height = 0.5)\n\np &lt;- ggplot(expertiseVknowsource_tib, aes(\n    x = know_source, \n    y = as.numeric(expertise), \n    color = know_source)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.6) +\n  geom_jitter(position = jitter, alpha = 0.2) +\n  scale_y_continuous(breaks = seq(0, 10, by = 1)) +\n  labs(\n    title = \"Distribution of Knowledge of Source by Self-Reported Expertise\",\n    x = \"Do You Know Where Your Coffee Comes From?\",\n    y = \"Claimed Level of Coffee Expertise\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np",
    "crumbs": [
      "Tidy Tuesday 2024",
      "20"
    ]
  },
  {
    "objectID": "tidytuesday2024/tidy20.html#results",
    "href": "tidytuesday2024/tidy20.html#results",
    "title": "20",
    "section": "Results",
    "text": "Results\nNot too suprising. The more you claim to know about coffee the more likely you are to also claim to know where your coffee comes from.\nBut who are those four coffee geniuses who don’t care where their coffee comes from?\n\nset.seed(357)\n\np_hilite &lt;- p +\n  geom_mark_ellipse(aes(fill = know_source,\n                        filter = (know_source == \"no\") & (expertise == 10),\n                        description = \"Experts\"),\n                    label.fill = NA,\n                    label.colour = \"darkgrey\",\n                    label.buffer = unit(0, \"mm\"),\n                    con.border = \"all\",\n                    con.colour = \"darkgrey\",\n                    con.cap = 0,\n                    position = jitter\n  )\n\np_hilite\n\n\n\n\n\n\n\n\n\nDrill Down\nI must know more about them\n\nexpVsource_outliers &lt;- strip_tib %&gt;%\n  mutate(expertise = as.integer(expertise)) %&gt;%\n  filter(expertise == 10) %&gt;%\n  filter(str_detect(know_source, \"no\"))\n\nkable(\n  expVsource_outliers\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsubmission_id\nage\ncups\nfavorite\nstyle\nstrength\nroast_level\ncaffeine\nexpertise\nwfh\nmonthly_expenditure\ntaste\nknow_source\nmost_paid\nmost_willing\nvalue_cafe\nspent_equipment\nvalue_equipment\ngender\neducation_level\nethnicity_race\nemployment_status\nnumber_children\npolitical_affiliation\n\n\n\n\nxeb6pd\n18-24 years old\n2\namericano\ncaramalized\nvery strong\nlight\nfull caffeine\n10\ni primarily work from home\n&lt;$20\nyes\nno\n$6-$8\n$4-$6\nyes\n$100-$300\nyes\nfemale\nbachelor’s degree\nasian/pacific islander\nemployed full-time\nnone\nno affiliation\n\n\npabjb0\n25-34 years old\n1\nregular drip coffee\nchocolatey\nsomewhat strong\ndark\nfull caffeine\n10\ni primarily work from home\n$40-$60\nyes\nno\n$10-$15\n$15-$20\nyes\n$20-$50\nyes\nfemale\nbachelor’s degree\nasian/pacific islander\nemployed full-time\nnone\nno affiliation\n\n\noa0agp\n18-24 years old\n1\nlatte\nchocolatey\nvery strong\nmedium\nfull caffeine\n10\ni primarily work in person\n$60-$80\nyes\nno\n$6-$8\n$6-$8\nyes\n$50-$100\nyes\nfemale\nbachelor’s degree\nasian/pacific islander\nemployed full-time\nnone\nindependent\n\n\nep8dr2\n25-34 years old\n1\npourover\nchocolatey\nsomewhat strong\ndark\nfull caffeine\n10\ni primarily work from home\n&lt;$20\nyes\nno\n$8-$10\n$10-$15\nyes\n$300-$500\nyes\nmale\nbachelor’s degree\nwhite/caucasian\nemployed full-time\nnone\nindependent\n\n\n\n\n\n\n\nResponsible Analysis\nLet’s remind ourselves that we left sound analysis practices behind at this point. We are simply having fun. These are outliers and ZERO conclusions can be drawn from digging into their data. So lets just stop here.",
    "crumbs": [
      "Tidy Tuesday 2024",
      "20"
    ]
  }
]